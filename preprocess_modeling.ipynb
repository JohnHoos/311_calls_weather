{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import cufflinks as cf\n",
    "import plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "import random\n",
    "import plotly.io as pio\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "cf.go_offline() # required to use plotly offline (no account required).\n",
    "py.init_notebook_mode() # graphs charts inline (IPython)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read datasets and join on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('weather_cleaned.csv')\n",
    "call_df = pd.read_csv('311_filtered.csv')\n",
    "\n",
    "call_df.columns = call_df.columns.str.replace(' ', '_')\n",
    "call_df.columns = call_df.columns.str.lower()\n",
    "\n",
    "# keep only date of fatetime\n",
    "weather_df['Date'] = pd.to_datetime(weather_df['Date']).dt.date\n",
    "call_df['created_date'] = pd.to_datetime(call_df['created_date'])\n",
    "call_df['created_timestamp'] = call_df['created_date']\n",
    "call_df['created_date'] = call_df['created_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['WBAN'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in weather_df['WBAN'].unique():\n",
    "#     print(len(weather_df[weather_df['WBAN']==i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df[weather_df['WBAN']==465]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate weather data by date and take average\n",
    "weather_bydate_df = weather_df.iloc[:,4:].groupby('Date').mean()\n",
    "\n",
    "# left join on date\n",
    "combined_df = call_df.merge(weather_bydate_df, left_on='created_date', right_on='Date', how='left')\n",
    "\n",
    "combined_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Build time series train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "# y = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "\n",
    "# tscv = TimeSeriesSplit()\n",
    "# print(tscv)\n",
    "# TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
    "# for train_index, test_index in tscv.split(X):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     print (\"X_train\", X_train, \"X_test\", X_test)\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_regression_analysis(X_train, y_train, X_test, y_test, method, alpha, L1_wt, regularized=False):\n",
    "    '''\n",
    "    Takes in a dataframe (X, y)\n",
    "    X - independent features\n",
    "    y - target, number of daily 311 calls\n",
    "    Returns regression summary\n",
    "    '''\n",
    "    \n",
    "    # scale data\n",
    "    col_names = X_train.columns\n",
    "    transformer = RobustScaler().fit(X_train)\n",
    "    X_train_transformed = pd.DataFrame(transformer.transform(X_train), columns=col_names)\n",
    "#     print ('X train transformed shape', X_train_transformed.shape)\n",
    "#     print ('y train  shape', y_train.values.reshape(-1,1).shape)\n",
    "#     print(\"X_train_trans\", X_train_transformed[0])\n",
    "    X_test_transformed = pd.DataFrame(transformer.transform(X_test), columns=col_names)\n",
    "#     print ('X test transformed shape', X_test_transformed.shape)\n",
    "    \n",
    "    #fitting the model\n",
    "    \n",
    "    # unregularized\n",
    "    if not regularized:\n",
    "#         print ('Running unregularized model...')\n",
    "        model= sm.OLS(y_train.values.reshape(-1,1), X_train_transformed).fit()         \n",
    "        #summary of the model\n",
    "        summary= model.summary()\n",
    "\n",
    "        pred = model.predict(X_test_transformed)\n",
    "        \n",
    "        mse_result = mean_squared_error(y_test, pred)\n",
    "        mae_result = mean_absolute_error(y_test, pred)\n",
    "        \n",
    "    # regularized\n",
    "    else:\n",
    "#         print ('Running regularized model...')\n",
    "        model= sm.OLS(y_train.values.reshape(-1,1), X_train_transformed).fit_regularized(method=method, alpha=alpha,\\\n",
    "                                                                                         L1_wt=L1_wt, refit=True)      \n",
    "        #summary of the model\n",
    "        summary= model.summary()\n",
    "\n",
    "        pred = model.predict(X_test_transformed)\n",
    "        mse_result = mean_squared_error(y_test, pred)\n",
    "        mae_result = mean_absolute_error(y_test, pred)\n",
    "        \n",
    "    return summary, mse_result, mae_result, pred\n",
    "   \n",
    "\n",
    "def timeseries_train_test_split(X, y, date, n_splits=5, max_train_size=60, test_size = 7, regularized=False, method='elastic_net', alpha=1.0, L1_wt=0.5):\n",
    "    if not regularized:\n",
    "        print ('Running unregularized model...')\n",
    "    else:\n",
    "        print ('Running regularized model...')\n",
    "        \n",
    "    performance_ls = []\n",
    "    summary_ls = []\n",
    "    pred_ls = []\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, max_train_size=max_train_size, test_size = test_size)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "#         print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        date_index = date[test_index]\n",
    "        \n",
    "        summary, mse, mae, pred = multivariate_regression_analysis(X_train, y_train, X_test, y_test,\\\n",
    "                                                             regularized=regularized, method=method, alpha=alpha, L1_wt=L1_wt)\n",
    "#         print(type(date_index))\n",
    "#         print(type(pred))\n",
    "        \n",
    "        performance_ls.append([mse, mae])\n",
    "        summary_ls.append(summary)\n",
    "        \n",
    "        pred_dict = {'date':date_index.values, 'pred':pred.values}\n",
    "        pred_ls.append(pd.DataFrame(pred_dict))\n",
    "        \n",
    "    return performance_ls, summary_ls, pred_ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_table(ls):\n",
    "    res = pd.DataFrame(np.array(performance_ls), columns=['MSE', 'MAE'])\n",
    "    res['RMSE'] = np.sqrt(res['MSE'])\n",
    "    \n",
    "    return res\n",
    "\n",
    "def plot_results(df, col, title):\n",
    "    fig = px.line(df, x=df.index, y=col)\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': title,\n",
    "            'y':0.9,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "            yaxis_range = [0,10_000])\n",
    "    fig.show()\n",
    "\n",
    "def pred_v_actual_plot(df, pred_df):\n",
    "\n",
    "    # visualize the time range and call volume\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=df['created_date'], y=df['num_calls'], name='Num of Calls',\n",
    "                            ))\n",
    "    fig.add_trace(go.Scatter(x=pred_df['date'], y=pred_df['pred'], name='Predicted Num of Calls',\n",
    "                            ))\n",
    "\n",
    "\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': \"Predicted v.s. Actual 311 Calls\",\n",
    "            'y':0.9,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'})\n",
    "    fig.show()\n",
    "\n",
    "def calc_avg_error(df):\n",
    "    print ('Mean absolute error (average): ', df['MAE'].mean())\n",
    "    print ('Root mean squared error (average): ', df['RMSE'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple regression model to predict total daily call volumnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df['incident_zip'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove certain features \n",
    "Based on previous EDA on the two datasets, we can drop based on correlation (multicolinearity), percentage of missing values, and domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped: 'agency_name', 'Gust', 'SnowDepth', 'resolution_description', 'resolution_action_updated_date'\n",
    "# 'incident_address', 'street_name', 'cross_street_1', 'cross_street_2', 'closed_date', 'created_timestamp'\n",
    "# 'hour', 'month', 'day', 'complaint_type', 'park_facility_name', 'city', 'status', 'latitude', 'longitude', 'location'\n",
    "\n",
    "col_ls = ['unique_key', 'created_date', 'agency',\n",
    "       'location_type', 'incident_zip',  'address_type',\n",
    "       'community_board', 'borough', 'open_data_channel_type', 'park_borough',\n",
    "       'complaint_type_original', \n",
    "       'day_of_week', 'Latitude',\n",
    "       'Longitude', 'MeanTemp',\n",
    "       'Percipitation', 'WindSpeed', 'MaxSustainedWind', 'Rain',\n",
    "       'SnowIce', 'Month', 'Day']\n",
    "\n",
    "filtered_df = combined_df[col_ls]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple regression with weather variables only\n",
    "Using exog variables only and not consider the autocorrelation of call volumes by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_df = filtered_df.groupby('created_date').agg({'unique_key':'size', 'MeanTemp':'mean','Percipitation':'mean', 'WindSpeed':'mean', 'MaxSustainedWind':'mean', 'Rain':'mean', 'SnowIce':'mean', 'Month':'mean', 'Day':'mean', 'day_of_week':'mean', 'Latitude':'mean', 'Longitude':'mean'})\\\n",
    "        .rename(columns={'unique_key':'num_calls','MeanTemp':'mean_temp', 'Percipitation':'mean_precip', 'WindSpeed':'mean_ws', 'MaxSustainedWind':'mean_max_ws', 'Rain':'rain', 'SnowIce':'snowice', 'Month':'month', 'Day':'day', 'day_of_week':'day_of_wk', 'Latitude':'lat', 'Longitude':'lon'}) \\\n",
    "        .reset_index()\n",
    "\n",
    "regress_df.dropna(inplace=True)\n",
    "print(regress_df.shape)\n",
    "\n",
    "# print (regress_df.shape)\n",
    "regress_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_df['num_calls'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=4, cols=3, shared_yaxes=True)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['mean_temp'], y=regress_df['num_calls'], name='mean temp',mode = 'markers'),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['mean_precip'], y=regress_df['num_calls'], name='mean precipitation',mode = 'markers'),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['mean_ws'], y=regress_df['num_calls'], name='mean wind speed',mode = 'markers'),\n",
    "              row=1, col=3)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['mean_max_ws'], y=regress_df['num_calls'], name='max wind speed',mode = 'markers'),\n",
    "              row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['rain'], y=regress_df['num_calls'], name='rain',mode = 'markers'),\n",
    "              row=2, col=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['snowice'], y=regress_df['num_calls'], name='snpw/ice',mode = 'markers'),\n",
    "              row=2, col=3)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['year'], y=regress_df['num_calls'], name='year',mode = 'markers'),\n",
    "              row=3, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['month'], y=regress_df['num_calls'], name='month',mode = 'markers'),\n",
    "              row=3, col=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['day'], y=regress_df['num_calls'], name='day',mode = 'markers'),\n",
    "              row=3, col=3)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['day_of_wk'], y=regress_df['num_calls'], name='day of week',mode = 'markers'),\n",
    "              row=4, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['lat'], y=regress_df['num_calls'], name='latitude',mode = 'markers'),\n",
    "              row=4, col=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['lon'], y=regress_df['num_calls'], name='longitude',mode = 'markers'),\n",
    "              row=4, col=3)\n",
    "\n",
    "fig.update_layout(height=600, width=600,\n",
    "                  title_text=\"Multiple Subplots with Call Volumes\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calls and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter(regress_df, x='created_date', y=\"num_calls\", trendline=\"ols\")\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "# visualize the time range and call volume\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['created_date'], y=regress_df['num_calls'], name='Num of Calls',\n",
    "                        yaxis='y1'\n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=regress_df['created_date'], y=regress_df['mean_temp'], name='Avg Temperature',\n",
    "                        yaxis='y2'\n",
    "                        ))\n",
    "# fig.add_trace(go.Scatter(x=regress_df['created_date'], y=regress_df['max_temp'], name='Max Temperature',\n",
    "#                         yaxis='y2'\n",
    "#                         ))\n",
    "\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Daily 311 Calls\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "        yaxis=dict(title='Calls'),\n",
    "                       yaxis2=dict(title='Temperature',\n",
    "                                   overlaying='y',\n",
    "                                   side='right'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calls and wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter(regress_df, x='created_date', y=\"num_calls\", trendline=\"ols\")\n",
    "# fig.show()\n",
    "\n",
    "regress_df.dropna(inplace=True)\n",
    "print(regress_df.shape)\n",
    "\n",
    "# visualize the time range and call volume\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['created_date'], y=regress_df['num_calls'], name='Num of Calls',\n",
    "                        yaxis='y1'\n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=regress_df['created_date'], y=regress_df['mean_ws'], name='Avg Wind Speed',\n",
    "                        yaxis='y2'\n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=regress_df['created_date'], y=regress_df['mean_max_ws'], name='Max Wind Speed',\n",
    "                        yaxis='y2'\n",
    "                        ))\n",
    "\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Daily 311 Calls\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "        yaxis=dict(title='Calls'),\n",
    "                       yaxis2=dict(title='Wind Speed',\n",
    "                                   overlaying='y',\n",
    "                                   side='right'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calls and precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter(regress_df, x='created_date', y=\"num_calls\", trendline=\"ols\")\n",
    "# fig.show()\n",
    "\n",
    "regress_df.dropna(inplace=True)\n",
    "print(regress_df.shape)\n",
    "\n",
    "# visualize the time range and call volume\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=regress_df['created_date'], y=regress_df['num_calls'], name='Num of Calls',\n",
    "                        yaxis='y1'\n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=regress_df['created_date'], y=regress_df['rain'], name='Rain',\n",
    "                        yaxis='y2'\n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=regress_df['created_date'], y=regress_df['snowice'], name='Snow/Ice',\n",
    "                        yaxis='y2'\n",
    "                        ))\n",
    "\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Daily 311 Calls\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "        yaxis=dict(title='Calls'),\n",
    "                       yaxis2=dict(title='Precipitation',\n",
    "                                   overlaying='y',\n",
    "                                   side='right'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 90-Day look back window, unregularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = (len(regress_df)-90)//7\n",
    "\n",
    "performance_ls, summary_ls, pred_ls = timeseries_train_test_split(regress_df.iloc[:,2:], regress_df.num_calls, regress_df.created_date,\\\n",
    "                                                         n_splits=n_splits, max_train_size=90, test_size = 7)\n",
    "\n",
    "res = results_table(performance_ls)\n",
    "display(summary_ls[-1])\n",
    "\n",
    "plot_results(res, [\"MAE\", 'RMSE'], 'Look Foward 7 Day 311 Call Volume Performance, 90-Day Window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_v_actual_plot(regress_df, pd.concat(pred_ls).reset_index(drop=True))\n",
    "calc_avg_error(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding window, unregularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_ls, summary_ls, pred_ls = timeseries_train_test_split(regress_df.iloc[:,2:], regress_df.num_calls, date=regress_df.created_date,\\\n",
    "                                                         n_splits=n_splits, max_train_size=None, test_size = 7)\n",
    "\n",
    "res = results_table(performance_ls)\n",
    "display(summary_ls[-1])\n",
    "\n",
    "plot_results(res, [\"MAE\", 'RMSE'], 'Look Foward 7 Day 311 Call Volume Performance, Expanding Window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_v_actual_plot(regress_df, pd.concat(pred_ls).reset_index(drop=True))\n",
    "calc_avg_error(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding window, regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_ls, summary_ls, pred_ls = timeseries_train_test_split(regress_df.iloc[:,2:], regress_df.num_calls, date=regress_df.created_date,\\\n",
    "                                                         n_splits=n_splits, max_train_size=None, test_size = 7, regularized=True, alpha=100)\n",
    "\n",
    "res = results_table(performance_ls)\n",
    "display(summary_ls[-1])\n",
    "\n",
    "plot_results(res, [\"MAE\", 'RMSE'], 'Look Foward 7 Day 311 Call Volume Performance, Expanding Window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_v_actual_plot(regress_df, pd.concat(pred_ls).reset_index(drop=True))\n",
    "calc_avg_error(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the following variables are satistically significant:\n",
    "- Mean temperature\n",
    "- Mean precipitation\n",
    "- Mean wind speed\n",
    "- Mean max wind speed\n",
    "- Rain (binary)\n",
    "- Snow/ice (binary)\n",
    "- Month\n",
    "- Day of week\n",
    "\n",
    "Whereas 'Precipitation' is related to 'Rain' and 'SnowIce' combined, removing them leads to a material degradation in performance, and the correlation between 'Precipitation', 'Rain' and 'SnowIce' is weak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurize total call volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_time_series(df_in):\n",
    "    df = df_in.copy()\n",
    "    # create lag features\n",
    "    df['lag_7'] = df['num_calls'].shift(7)\n",
    "    df['lag_7'].fillna(method='bfill', inplace=True)\n",
    "    df['lag_8'] = df['num_calls'].shift(8)\n",
    "    df['lag_8'].fillna(method='bfill', inplace=True)\n",
    "    df['lag_9'] = df['num_calls'].shift(9)\n",
    "    df['lag_9'].fillna(method='bfill', inplace=True)\n",
    "    df['lag_14'] = df['num_calls'].shift(14)\n",
    "    df['lag_14'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # trailing average\n",
    "    df['moving_avg7'] = df['num_calls'].shift(7).rolling(window=7, min_periods=1).mean()\n",
    "    df['moving_avg7'].fillna(method='bfill', inplace=True)\n",
    "    df['moving_avg30'] = df['num_calls'].shift(7).rolling(window=30, min_periods=1).mean()\n",
    "    df['moving_avg30'].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    # cumulative moving average\n",
    "    df['cm_avg'] = df['num_calls'].shift(7).expanding(min_periods=1).mean()\n",
    "    df['cm_avg'].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    # exp moving average\n",
    "    df['exp_avg'] = df['num_calls'].shift(7).ewm(com=0.5, min_periods=1).mean()\n",
    "    df['exp_avg'].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_featurized_df = featurize_time_series(regress_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_featurized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series with weather variables, unregularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(regress_featurized_df.shape)\n",
    "\n",
    "performance_ls, summary_ls, pred_ls = timeseries_train_test_split(regress_featurized_df.iloc[:,2:], regress_featurized_df.num_calls, date=regress_featurized_df.created_date,\\\n",
    "                                                         n_splits=n_splits, max_train_size=None, test_size = 7)\n",
    "\n",
    "\n",
    "res = results_table(performance_ls)\n",
    "display(summary_ls[-1])\n",
    "\n",
    "plot_results(res, [\"MAE\", 'RMSE'], 'Look Foward 7 Day 311 Call Volume Performance, Expanding Window w/ Historical Call Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_v_actual_plot(regress_featurized_df, pd.concat(pred_ls).reset_index(drop=True))\n",
    "calc_avg_error(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series with weather variables, regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_ls, summary_ls, pred_ls = timeseries_train_test_split(regress_featurized_df.iloc[:,2:], regress_featurized_df.num_calls, date=regress_featurized_df.created_date,\\\n",
    "                                                         n_splits=n_splits, max_train_size=None, test_size = 7, regularized=True, alpha=100)\n",
    "\n",
    "res = results_table(performance_ls)\n",
    "display(summary_ls[-1])\n",
    "\n",
    "plot_results(res, [\"MAE\", 'RMSE'], 'Look Foward 7 Day 311 Call Volume Performance, Expanding Window w/ Historical Call Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_v_actual_plot(regress_featurized_df, pd.concat(pred_ls).reset_index(drop=True))\n",
    "calc_avg_error(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a tree-based model\n",
    "### Random forest with weather and autoregressive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_analysis(X_train, y_train, X_test, y_test, n_estimators, max_depth, min_samples_split):\n",
    "    '''\n",
    "    Takes in a dataframe (X, y)\n",
    "    X - independent features\n",
    "    y - target, number of daily 311 calls\n",
    "    Returns regression summary\n",
    "    '''\n",
    "    \n",
    "#     # scale data\n",
    "#     col_names = X_train.columns\n",
    "#     transformer = RobustScaler().fit(X_train)\n",
    "#     X_train_transformed = pd.DataFrame(transformer.transform(X_train), columns=col_names)\n",
    "\n",
    "#     X_test_transformed = pd.DataFrame(transformer.transform(X_test), columns=col_names)\n",
    "    \n",
    "    #fitting the model\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,\\\n",
    "                                  min_samples_split=min_samples_split, n_jobs=-1)\n",
    "    model = model.fit(X_train, y_train)\n",
    "\n",
    "    feats = {} # a dict to hold feature_name: feature_importance\n",
    "    for feature, importance in zip(X_train.columns, model.feature_importances_):\n",
    "        feats[feature] = importance #add the name/value pair \n",
    "    summary = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    mse_result = mean_squared_error(y_test, pred)\n",
    "    mae_result = mean_absolute_error(y_test, pred)\n",
    "        \n",
    "        \n",
    "    return summary, mse_result, mae_result, pred\n",
    "   \n",
    "\n",
    "def timeseries_train_test_split_forest(X, y, date, n_estimators, max_depth, min_samples_split,\\\n",
    "                                n_splits=5, max_train_size=60, test_size = 7):\n",
    "\n",
    "    performance_ls = []\n",
    "    summary_ls = []\n",
    "    pred_ls = []\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, max_train_size=max_train_size, test_size = test_size)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "#         print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        date_index = date[test_index]\n",
    "        \n",
    "        summary, mse, mae, pred = random_forest_analysis(X_train, y_train, X_test, y_test,\\\n",
    "                                                        n_estimators, max_depth, min_samples_split)\n",
    "#         print(type(date_index))\n",
    "#         print(type(pred))\n",
    "        \n",
    "        performance_ls.append([mse, mae])\n",
    "        summary_ls.append(summary)\n",
    "        \n",
    "        pred_dict = {'date':date_index.values, 'pred':pred}\n",
    "        pred_ls.append(pd.DataFrame(pred_dict))\n",
    "        \n",
    "    return performance_ls, summary_ls, pred_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_ls, summary_ls, pred_ls = timeseries_train_test_split_forest(regress_featurized_df.iloc[:,2:], regress_featurized_df.num_calls, date=regress_featurized_df.created_date, \\\n",
    "                                                                  n_estimators=120, max_depth=4, min_samples_split=2, n_splits=n_splits, max_train_size=None, test_size = 7)\n",
    "\n",
    "res = results_table(performance_ls)\n",
    "# display(summary_ls[-1])\n",
    "\n",
    "plot_results(res, [\"MAE\", 'RMSE'], 'Look Foward 7 Day 311 Call Volume Performance, Expanding Window w/ Historical Call Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = summary_ls[-1].sort_values(by='Gini-importance')\n",
    "fig = px.bar(plot, x=plot.index, y='Gini-importance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_v_actual_plot(regress_featurized_df, pd.concat(pred_ls).reset_index(drop=True))\n",
    "calc_avg_error(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest with autoregressive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_featurized_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators=[100, 150, 200]\n",
    "# max_depth=[4,6,8]\n",
    "# min_samples_split=[2,4,6,8]\n",
    "\n",
    "\n",
    "# param_grid = dict('n_estimators':n_estimators, 'max_depth':max_depth, 'min_sample_split':min_samples_split)\n",
    "\n",
    "performance_ls, summary_ls, pred_ls = timeseries_train_test_split_forest(regress_featurized_df.iloc[:,8:], regress_featurized_df.num_calls, date=regress_featurized_df.created_date, \\\n",
    "                                                                  n_estimators=120, max_depth=4, min_samples_split=2, n_splits=n_splits, max_train_size=None, test_size = 7)\n",
    "\n",
    "res = results_table(performance_ls)\n",
    "# display(summary_ls[-1])\n",
    "\n",
    "plot_results(res, [\"MAE\", 'RMSE'], 'Look Foward 7 Day 311 Call Volume Performance, Expanding Window w/ Historical Call Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = summary_ls[-1].sort_values(by='Gini-importance')\n",
    "fig = px.bar(plot, x=plot.index, y='Gini-importance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_v_actual_plot(regress_featurized_df, pd.concat(pred_ls).reset_index(drop=True))\n",
    "calc_avg_error(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an autoregressive time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from numpy import log\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "result = adfuller(regress_df.num_calls.dropna())\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n",
    "\n",
    "# Original Series\n",
    "fig, axes = plt.subplots(3, 2, sharex=True)\n",
    "axes[0, 0].plot(regress_df.num_calls); axes[0, 0].set_title('Original Series')\n",
    "axes[0,1].set(xlim=(0,30))\n",
    "plot_acf(regress_df.num_calls, ax=axes[0, 1])\n",
    "\n",
    "# 1st Differencing\n",
    "axes[1, 0].plot(regress_df.num_calls.diff()); axes[1, 0].set_title('1st Order Differencing')\n",
    "plot_acf(regress_df.num_calls.diff().dropna(), ax=axes[1, 1])\n",
    "\n",
    "# 2nd Differencing\n",
    "axes[2, 0].plot(regress_df.num_calls.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\n",
    "plot_acf(regress_df.num_calls.diff().diff().dropna(), ax=axes[2, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF plot of 1st differenced series\n",
    "plt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, sharex=True)\n",
    "axes[0,0].plot(regress_df.num_calls.diff()); axes[0,0].set_title('1st Differencing')\n",
    "axes[0,0].set(xlim=(0,30))\n",
    "plot_pacf(regress_df.num_calls.diff().dropna(), ax=axes[0, 1])\n",
    "\n",
    "axes[1,0].plot(regress_df.num_calls.diff().diff()); axes[1,0].set_title('2nd Differencing')\n",
    "axes[1,0].set(xlim=(0,30))\n",
    "plot_pacf(regress_df.num_calls.diff().diff().dropna(), ax=axes[1, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training data\n",
    "data = regress_df.num_calls\n",
    "# specify additional data\n",
    "other_data = regress_df.mean_temp\n",
    "\n",
    "# define model configuration\n",
    "my_order = (2, 1, 1)\n",
    "my_seasonal_order = (2, 1, 1, 7)\n",
    "# define model\n",
    "model = sm.tsa.statespace.SARIMAX(endog=data, exog=other_data, order=my_order, seasonal_order=my_seasonal_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model.fit(method='bfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # split into train and test sets\n",
    "# X = regress_df.num_calls\n",
    "# size = int(len(X) * 0.66)\n",
    "# train, test = X[0:size], X[size:len(X)]\n",
    "# history = [x for x in train]\n",
    "# predictions = []\n",
    "\n",
    "# # walk-forward validation\n",
    "# for t in range(size, len(X)):\n",
    "#     model = sm.tsa.statespace.SARIMAX(endog=data, exog=other_data, order=my_order, seasonal_order=my_seasonal_order)\n",
    "#     model_fit = model.fit(method='bfgs')\n",
    "#     output = model_fit.forecast()\n",
    "#     yhat = output[0]\n",
    "#     predictions.append(yhat)\n",
    "#     obs = test[t]\n",
    "#     history.append(obs)\n",
    "#     print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "    \n",
    "# # evaluate forecasts\n",
    "# rmse = np.sqrt(mean_squared_error(test, predictions))\n",
    "# print('Test RMSE: %.3f' % rmse)\n",
    "# # plot forecasts against actual outcomes\n",
    "# plt.plot(test)\n",
    "# plt.plot(predictions, color='red')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_df.head()\n",
    "\n",
    "arima_df = pd.DataFrame()\n",
    "arima_df['date'] = pd.to_datetime(regress_df['created_date'])\n",
    "arima_df['mean_temp'] = regress_df['mean_temp']\n",
    "arima_df['num_calls'] = regress_df['num_calls']\n",
    "arima_df.set_index('date', inplace=True)\n",
    "\n",
    "arima_df.index = pd.DatetimeIndex(arima_df.index.values,\n",
    "                               freq=arima_df.index.inferred_freq)\n",
    "\n",
    "ts = arima_df['num_calls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts, label='prediction seasonal')\n",
    "plt.grid()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(ts, model='additive',extrapolate_trend='freq')\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(ts):\n",
    "    dftest = adfuller(ts)\n",
    "    adf = dftest[0]\n",
    "    pvalue = dftest[1]\n",
    "    critical_value = dftest[4]['5%']\n",
    "    if (pvalue < 0.05) and (adf < critical_value):\n",
    "        print('The series is stationary')\n",
    "    else:\n",
    "        print('The series is NOT stationary')\n",
    "        \n",
    "seasonal = result.seasonal\n",
    "check_stationarity(seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train = arima_df[:'2018-03-01']['num_calls']\n",
    "ts_test = arima_df['2018-03-02':]['num_calls']\n",
    "\n",
    "model=sm.tsa.statespace.SARIMAX(endog=ts_train, exog=arima_df[:'2018-03-01'].mean_temp,order=(2, 1, 1),seasonal_order=(2,1,1,7))\n",
    "results=model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of fit model\n",
    "print(results.summary())\n",
    "# line plot of residuals\n",
    "residuals = pd.DataFrame(results.resid)\n",
    "residuals.plot()\n",
    "plt.show()\n",
    "# density plot of residuals\n",
    "residuals.plot(kind='kde')\n",
    "plt.show()\n",
    "# summary stats of residuals\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arima_df['forecast']=results.get_forecast('2018-03-02')\n",
    "out = pd.DataFrame(results.forecast(7, exog=arima_df['2018-03-02':'2018-03-08'].mean_temp))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the time range and call volume\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=arima_df.index, y=arima_df['num_calls'], name='Num of Calls',\n",
    "                        yaxis='y1'\n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=out.index, y=out['predicted_mean'], name='Prediction',\n",
    "                        yaxis='y1'\n",
    "                        ))\n",
    "\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Daily 311 Calls\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "        yaxis=dict(title='Calls'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarimax_analysis(endog_train, exog_train, endog_test, exog_test, order=(2,1,1), seasonal_order=(2,1,1,7)):\n",
    "    '''\n",
    "    Takes in a endogenous and exognenous variables, as well as other parameters\n",
    "\n",
    "    Returns regression summary\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #fitting the model\n",
    "    model = sm.tsa.statespace.SARIMAX(endog=endog_train, exog=exog_train,order=order,seasonal_order=seasonal_order, freq='D')\n",
    "    results=model.fit(disp=False)\n",
    "    pred = results.forecast(7, exog=exog_test)\n",
    "#     display(pred)\n",
    "    summary = results.summary()\n",
    "    \n",
    "    mse_result = mean_squared_error(endog_test, pred)\n",
    "    mae_result = mean_absolute_error(endog_test, pred)\n",
    "        \n",
    "        \n",
    "    return summary, mse_result, mae_result, pred\n",
    "   \n",
    "\n",
    "def timeseries_train_test_split_sarimax(endog, exog, date, order=(2,1,1), seasonal_order=(2,1,1,7),\\\n",
    "                                n_splits=120, max_train_size=90, test_size = 7):\n",
    "\n",
    "    performance_ls = []\n",
    "    summary_ls = []\n",
    "    pred_ls = []\n",
    "    cnt = 0\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, max_train_size=max_train_size, test_size = test_size)\n",
    "    for train_index, test_index in tscv.split(endog):\n",
    "        cnt += 1\n",
    "        if cnt % 10 == 0:\n",
    "            print('Training {} split'.format(cnt))\n",
    "        endog_train, endog_test = endog[train_index], endog[test_index]\n",
    "        exog_train, exog_test = exog[train_index], exog[test_index]\n",
    "\n",
    "        date_index = date[test_index]\n",
    "        \n",
    "        summary, mse, mae, pred = sarimax_analysis(endog_train=endog_train, exog_train=exog_train, endog_test=endog_test, exog_test=exog_test,\\\n",
    "                                                   order=order, seasonal_order=seasonal_order)\n",
    "#         display(pred)\n",
    "\n",
    "        pred_df = pd.DataFrame(pred)\n",
    "        performance_ls.append([mse, mae])\n",
    "        summary_ls.append(summary)\n",
    "        \n",
    "        pred_ls.append(pred_df)\n",
    "        \n",
    "    return performance_ls, summary_ls, pred_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog = arima_df['num_calls']\n",
    "exog = arima_df['mean_temp']\n",
    "date = arima_df.index\n",
    "performance_ls, summary_ls, pred_ls = timeseries_train_test_split_sarimax(endog, exog, date, order=(1,1,1), seasonal_order=(1,1,1,7),\\\n",
    "                                n_splits=120, max_train_size=None, test_size = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = results_table(performance_ls)\n",
    "# display(summary_ls[-1])\n",
    "\n",
    "plot_results(res, [\"MAE\", 'RMSE'], 'Look Foward 7 Day 311 Call Volume Performance, Expanding Window w/ Historical Call Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(np.percentile(arima_df.num_calls, 15))\n",
    "display(np.percentile(arima_df.num_calls, 85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(arima_df.num_calls.min())\n",
    "display(arima_df.num_calls.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_v_actual_plot(df, pred_df):\n",
    "\n",
    "    # visualize the time range and call volume\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['num_calls'], name='Num of Calls',\n",
    "                            ))\n",
    "    fig.add_trace(go.Scatter(x=pred_df.index, y=pred_df['predicted_mean'], name='Predicted Num of Calls',\n",
    "                            ))\n",
    "\n",
    "\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': \"Predicted v.s. Actual 311 Calls\",\n",
    "            'y':0.9,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'})\n",
    "    fig.show()\n",
    "\n",
    "pred_v_actual_plot(arima_df, pd.concat(pred_ls))\n",
    "calc_avg_error(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
